% Writeup for UCSD LIGN 199, Winter 2018.
% Student: Xiaoqing (Tom) Li, xil056@ucsd.edu
% Project leader: Matthew Zaslansky
% .tex structure based off sample from ftp://www.ctan.org/tex-archive/macros/latex/base/sample2e.tex

\documentclass{article}      % Specifies the document class
% The preamble begins here.
\title{Winter 2018 LIGN 199 Writeup}  % Declares the document's title.
\author{Xiaoqing (Tom) Li}      % Declares the author's name.
\date\today      % Deleting this command produces today's date.
\usepackage{xcolor}
\usepackage{minted}
\definecolor{bg}{rgb}{0.95,0.95,0.95}

\begin{document}             % End of preamble and beginning of text.
	
	\maketitle                   % Produces the title.
	
	In Winter Quarter of 2018, I was enrolled in LIGN 199, a special independent undergraduate research course. I assisted Matthew Zaslansky, a graduate student at UCSD, with research in Azerbaijani, chiefly data compilation from an online corpus, SketchEngine, and processing of that data, in Python. All code from the project can be found in my GitHub repository, https://github.com/variablehair/199wi18. The course is graded as P/NP and is worth four credits.
	
	\section{Introduction}
	
	A relatively small number of external packages and modules were used in this project; they will be briefly introduced here.
	
	\subsection{\texttt{json}}
	
	The \texttt{json} package is used widely throughout the project as one of the two primary data management formats. Primarily, it is used to save and load data to disk in a machine-friendly format.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	r = s.get(url, params = attrs)
	curr_res_dict = r.json()
	...
	with open('data/temp/' + query + '100.json', mode='w', encoding='utf=8') as f:
		json.dump(curr_results, f)
	\end{minted}
	
	In the above snippet from \texttt{data\_fetch.py}, data is retrieved from SketchEngine in JSON format, and after processing, is then stored in a \texttt{.json} file.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	with open('data/dative_pruned.json') as f:
		j_dat = json.load(f)
	
	for token in j_dat:
		...
	\end{minted}
	
	In the above snippet from \texttt{xlsx\_writer\_task2.py}, stored JSON data is loaded into the program. This approach, while simple, ran into a problem of magnitude; specifically, \texttt{json.load} was found to have significant overhead on memory (RAM) which seemed to scale exponentially with file size, rendering this approach untenable when the data set became large, as ours did.
	
	\subsection{\texttt{sqlite3}}
	
	As a solution to this problem, the dataset was transposed to a SQLite database using the \texttt{sqlite3} package. \texttt{sqlite3} was chosen because of its minimal setup and upkeep, making it ideal for our task. Its primary drawback is its relatively slower speed, but as our total and average throughput of database access was extremely low, this was not a concern.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	conn = sqlite3.connect('data/data100.db')
	...
	def process_raw_json(filename, tokenname, dbconn):
		c = dbconn.cursor()
	try:
		c.execute('''CREATE TABLE {} (token TEXT, left TEXT, right TEXT)'''.format(tokenname))
		...
	for string_dict in string_list:
		query = '''INSERT INTO {0} VALUES ('{1}','{2}','{3}')'''.format(tokenname, result_dict['Kwic'][0]['str'], left_str.replace("\'", "\'\'"), right_str.replace("\'", "\'\'"))
		try:
			c.execute(query)
		...
	\end{minted}
	
	In the above snippet from \texttt{compile\_data100.py}, tables are created in our database and populated with data using database connection and cursor objects.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	c.execute('''SELECT token, left, right FROM {0}'''.format(token))
	
	while True:
		r = c.fetchone()
		if r is None:
			break
		word, left, right = r
		...
	\end{minted}
	
	In the above snippet from \texttt{counter\_task2\_dict.py}, a query which fetches every row from a particular table is executed. By using \texttt{c.fetchone()}, we only load a single row of data at a time into memory, bypassing the JSON problem (wherein the entire dataset had to be loaded into memory).
	
	\subsection{\texttt{xlsxwriter}}
	
	The \texttt{xlsxwriter} package is used several times throughout the project to generate human-readable \texttt{.xlsx} files once data had been processed. It was chosen from several options because it was the most lightweight due to lacking the functionality to open existing \texttt{.xlsx} files. However, this functionality was not required in this project.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	workbook_dat = xlsxwriter.Workbook('dict_pruned_dative.xlsx')
	...
	for token in j_dat:
		worksheet = workbook_dat.add_worksheet(token)
		row = 0
		...
		for tup in tup_results:
			worksheet.write(row, 0, word)
			worksheet.write(row, 1, tup[0])
			worksheet.write(row, 2, tup[1])
		row += 1
	workbook_dat.close()
	\end{minted}
	
	In the above snippet from \texttt{xlsx\_writer\_task2.py}, \texttt{xlsxwriter} is used to create a file (\texttt{Workbook}), sheets (\texttt{worksheet}s), and then write particular results iteratively to each row.
	
	\subsection{\texttt{string}}
	
	The \texttt{string} package is used to access various built-in constants, such as a list of all the punctuation.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	import string
	...
		word = word.strip(string.punctuation + string.digits + '´')
		...
	\end{minted}
	
	In the above snippet from \texttt{simple\_get\_dict\_entry.py}, \texttt{string.punctuation} and \texttt{string.digits} are used as arguments to \texttt{strip()} in order to remove trailing punctuation marks and numerals from dictionary entries as part of the process to convert them to clean, machine-readable format.
	
	\subsection{\texttt{re}}
	
	The regular expression package \texttt{re} is used to perform more advanced trimming operations on the dictionary entries.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	try:
		if line[0].isupper():
		curr = re.sub(r'\([^()]*\)', '', line[0])
		...
	\end{minted}
	
	In the above snippet from \texttt{syntactic\_category\_fetch.py}, a regular expression is used to remove all strings between parentheses within words, as such strings were commonly found in the dictionary.
	
	\subsection{\texttt{requests} and \texttt{urllib.parse}}
	
	The \texttt{requests} package and \texttt{urllib.parse} module are used only in the data fetching process to interface with SketchEngine's API.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	s = requests.Session()
	s.auth = (tom_username, tom_password)
	s.get(login_url)
	url = base_url + 'view'
	...
	r = s.post(login_url,data=logindata)
	encoded_attrs=urllib.parse.urlencode(attrs)
	r = s.get(url, params = attrs)
	\end{minted}
	
	\texttt{requests} is used in the above snippet from \texttt{data\_fetch.py} to create an interface with SketchEngine via a \texttt{Session} object, which sends and receives data with \texttt{post} and \texttt{get}. \texttt{urllib.parse} is used to encode the data sent to the server into the appropriate format.
	
	\subsection{\texttt{time}}
	
	The \texttt{time} package is used only in the data fetching process to delay execution of the code in order to comply with SketchEngine's rate limits.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	while pagenum < MAX_SAMPLES:
	pagenum += 1
	attrs['fromp'] = pagenum
	r = s.get(url, params=attrs)
	curr_res_dict = r.json()
	...	
	time.sleep(10)
	if pagenum % 25 == 0:
	time.sleep(60)
	\end{minted}
	
	In the above snippet from \texttt{data\_fetch.py}, \texttt{time.sleep} is used to pause execution of the loop on every iteration for 10 seconds with an additional pause on every 25th iteration for 60 seconds. This was found to be within the limits allowed by SketchEngine despite being less than their prescribed pause (44 seconds per query), which is prohibitively long for our task.
	
	\subsection{\texttt{os} and \texttt{ast}}
	
	\texttt{os} and \texttt{ast} used only in \texttt{compile\_data100.py} and are used to work with the filesystem and raw data respectively
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	datapath = 'data/temp/'
	datadir = os.fsencode(datapath)
	...
	for file in os.listdir(datadir):
		filename = os.fsdecode(file)
		tokenname = filename[0:-8]
		process_raw_json(datapath+filename, tokenname, conn)
	\end{minted}
	
	In the above snippet from \texttt{compile\_data100.py}, \texttt{os.fsencode} and \texttt{os.fsdecode} are used to allow Python to iterate through a folder containing raw data in JSON format.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	split_string = '{\"rightsize'
	string_list = [split_string + s[:-2] for s in raw_string.split(split_string) if len(s) >= 2]
	for string_dict in string_list:
		try:
			result_dict = ast.literal_eval(string_dict)
		except SyntaxError:
			result_dict = ast.literal_eval(string_dict + '}')
	\end{minted}
		
	In the above (somewhat hacky) snippet from \texttt{compile\_data100.py}, \texttt{ast.literal\_eval()} is used to convert a string to a Python data structure, in this case a dictionary. This approach was necessary because the files in question were too large to load directly despite being in JSON format, so they had to be loaded as strings before being converted.
	
	\subsection{\texttt{operator.itemgetter()}}
	
	The module \texttt{operator.itemgetter()} was used as an efficient alternative to the built-in \texttt{dict.get()} for efficiency reasons. Though it probably would not have made a tangible difference, our data sets could sometimes be very large, and so \texttt{itemgetter()} was used as a safeguard.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	results_list = sorted(found_words.items(), key=itemgetter(1), reverse=True)
	\end{minted}
	
	\texttt{itemgetter()} was used exclusively in \texttt{sorted()}, which produced a sorted list of tuples from a dictionary.
		
		%
		%END OF INTRODUCTION
		%
		
	\section{Structure and non-code files}
	
	A number of non-Python files (e.g. \texttt{.txt}, etc.) are found in the repository. They, along with the structure of the repository, will be very briefly detailed here.
	
	\subsection{Structure}
	
	The repository's structure is very simple. Most files are found at the top level. The directory \texttt{azer\_dictionary\_cleanup} contains files relevant to our work converting and working with an Azerbaijani dictionary, and the directory \texttt{data} contains data in various forms. However, note that the data directory is not actually available in the repository due to size constraints (e.g. the database is 267MB) despite being frequently used in the code. This means that trying to run the code directly after cloning the repository will result in errors; the data can, of course, be provided upon request. Lastly, unless otherwise noted, all files use UTF-8 encoding.
	
	\subsection{\texttt{dict.txt}, \texttt{dict\_entries.txt}, and \texttt{dict\_entries\_noparens.txt}}
	
	These three files contain the Azerbaijani dictionary and information from it in text format. \texttt{dict.txt} contains a plaintext version of the dictionary itself. \texttt{dict\_entries.txt} contains a roughly parsed and unpruned list of entries from the dictionary, and \texttt{dict\_entries\_noparens.txt} contains the same list, but with word-internal parenthesized blocks removed.
	
	\subsection{\texttt{task1clean.txt}}
	
	This probably poorly named file contains a list of all query strings (e.g. "dir", "ar", etc.) used in our project. It is called `clean' because it contains only the raw search strings without either artifacts from when this list was sent (e.g. preceding hyphens) or when these strings are used as actual search queries in the corpus (i.e. asterisks/regex formatting). The results from these queries comprised all of the data used, and this file is used in several places, e.g. to name the tables in the database.
	
		%
		% END OF STUCTURE
		%
	
	\section{Individual Python script overviews}
	
	In this section, the individual Python modules will be described in rough chronological order. Note that some will be deprecated (and marked as such), mostly due to the change from JSON to SQLite.
	
	%
	% data_fetch
	%
	
	\subsection{\texttt{data\_fetch.py}}
	
	\texttt{data\_fetch.py} interfaces with the SketchEngine API, sends queries, receives data, and then saves it to disk.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	from login import tom_username, tom_password
	import requests
	import json
	import urllib.parse
	import time
	
	MAX_SAMPLES = 100
	
	root_url = 'https://the.sketchengine.co.uk'
	base_url='%s/bonito/run.cgi/' % root_url
	
	login_url = 'https://the.sketchengine.co.uk/login/'
	logindata = {'username' : tom_username, 'password' : tom_password,'submit' : 'ok'}
	
	qlist = []
	with open('task1clean.txt', encoding='utf-8') as queries:
		for line in queries:
			qlist.append(line.replace('\n', ''))
	
	s = requests.Session()
	s.auth = (tom_username, tom_password)
	s.get(login_url)
	url = base_url + 'view'
	
	api_log = open('api_log.txt', encoding='utf-8', mode='w')
	\end{minted}
	
	Some general definitions and imports, part of which comes directly from SketchEngine's example script. \texttt{MAX\_SAMPLES} is probably poorly named, but is the maximum number of pages per query; there are 1,000 results per page, so this sets the limit for maximum number of results per query at 100,000. \texttt{qlist} is a list of queries that we are interested in. \texttt{api\_log} is a log file for debugging purposes. The remaining variables are various API-related operators, e.g. \texttt{logindata} is imported and then used to authorize the session, allowing us to make further requests.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	for query in qlist:
		pagenum = 1
		fquery = str('q[word=".*' + query + '.*"]')
		attrs = dict(corpname = 'preloaded/turkic_az', q = fquery, pagesize = '1000', format = 'json', fromp=pagenum, r='r1000')
		curr_results = list()
		
		r = s.post(login_url,data=logindata)
		
		encoded_attrs=urllib.parse.urlencode(attrs)
		
		r = s.get(url, params = attrs)
		curr_res_dict = r.json()
		
		try:
			e = (curr_res_dict['error'])
			api_log.write('ERROR on ' + str(query) + ':\t' + str(e) + '\n')
			continue
		except KeyError:
			pass
			
		curr_results = curr_res_dict['Lines']
		try:
			total_pages = curr_res_dict['numofpages']
		except KeyError:
			total_pages = 1
		api_log.write(query + ' page ' + str(pagenum) + 'done, len = ' + str(len(curr_results)) + '\n')
	\end{minted}
	
	This section sets up the \texttt{for} loop which encapsulates the rest of the file (keep in mind that the following sections are within the loop). A request is first constructed in the \texttt{attrs} object; note that pagesize is 1000, the maximum allowed. \texttt{r} is supposed to return a random sample of results according to the documentation, but it seems to not do this (or perhaps is incompatible with another option, but is not documented). \texttt{fromp} is the page that is returned, starting with 1, and in our case increasing to a maximum of 100.
	
	This request is then sent to the corpus with \texttt{requests.get()}, and the result is converted to a dictionary using the \texttt{json} library. The first thing that is checked is whether there is an error, either a rate limit or an empty result; if so, we move on to the next iteration (i.e. the next query) with \texttt{continue}. Finally, if there is no error, we save the current page of results (the data itself is found in \texttt{Lines}) and then count the total number of pages.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
		if total_pages > 1:
			while pagenum < MAX_SAMPLES:
				pagenum += 1
				attrs['fromp'] = pagenum
				r = s.get(url, params=attrs)
				curr_res_dict = r.json()
				curr_results = curr_results + curr_res_dict['Lines']
				api_log.write(query + ' page ' + str(pagenum) + 'done, len = ' + str(len(curr_results)) + '\n')			
				time.sleep(10)
				if pagenum % 25 == 0:
				time.sleep(60)
				print(str(pagenum))
		
		with open('data/temp/' + query + '100.json', mode='w', encoding='utf=8') as f:
		json.dump(curr_results, f)
		curr_results = []
		
		time.sleep(120)
	
	api_log.close()
	\end{minted}
	
	In this section, we essentially repeat the loop if there are multiple pages of results, up to our defined maximum. Note that there are multiple \texttt{time.sleep()} calls in order to avoid rate limits from SketchEngine. Finally, after all the data has been compiled, it is unceremoniously dumped to a JSON file in its raw form.
	
	\subsection{\texttt{counter\_task1.py}}
	
	Deprecated.
	
	This module was created for the first task, which was simply an ordered token count of words which contained query strings. It has not been updated with the new database. It also does not contain optimizations and improvements found in later scripts.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	import json
	import xlsxwriter
	
	f = open('data/task1.json')
	d = json.load(f)
	k = list(d.keys())
	
	workbook = xlsxwriter.Workbook('data.xlsx')
	
	for key in k:
		try:
			l = d[key]['Lines']
		except KeyError:
			continue
	
	results = dict()
	
	for r in l:
		try:
			word = r['Kwic'][0]['str']
			results[word] = results.get(word, 0) + 1
		except KeyError:
			continue
	
	worksheet = workbook.add_worksheet(key)
	
	row = 0
	
	for result in results.keys():
		row += 1
		worksheet.write(row, 0, result)
		worksheet.write(row, 1, results[result])
	
	workbook.close()
	\end{minted}
	
	This module tries to access the \texttt{Lines} object of the result JSON file, which contains a page of results. After that, it tries to access \texttt{r[`Kwic'][0][`str']}, which contains the word which contains the query string. A results dictionary then counts the number of occurrences of such words.
	
	\subsection{\texttt{char\_replace.py}}
	
	This module replaces characters from the custom encoding found in the Azerbaijani dictionary to the more common UTF-8.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	with open("in.txt", encoding="utf-8") as infile, open ("out.txt", mode="w", encoding="utf-8") as outfile:
		for line in infile:
			for char in line:
				try:
					out_char = char_dict[char]
					outfile.write(out_char)
				except KeyError:
					outfile.write(char)    
	\end{minted}
	
	Not included due to space is the dictionary which maps the replacement characters. This module iterates through the file and simply replaces all characters found in the dict; if the character is not found, it simply writes it with no changes.
	
	\subsection{\texttt{simple\_get\_dict\_entry.py}}
	
	This module attempts to extract a list of dictionary entries from the converted dictionary in text form.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	import string
	
	with open("dict.txt", encoding="utf-8") as infile, open("dict_entries.txt", encoding="utf-8", mode="w") as outfile:
		for line in infile:
			try:
				word = line.split(maxsplit=1)[0]
				if word.isupper():
					word = word.strip(string.punctuation + string.digits + '´')
					if len(word) >= 2:
						outfile.write(word + "\n")
			except IndexError:
				continue
	\end{minted}
	
	Each line is split at whitespace, and the first word is examined. If it is entirely in uppercase, and longer than 2 characters after punctuation or numerals have been removed, then it is added to the list of entries. Finer pruning was certainly possible but perhaps not needed for this task.
	
	\subsection{\texttt{compile\_data100.py}}
	
	This module takes the JSON data from \texttt{data\_fetch.py} and puts it into a SQLite database. Because the JSON data already existed, it was written to work with that specifically; a much better approach for any future implementation would be to have the data fetch module work directly with the database, eliminating this middle man.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	def process_raw_json(filename, tokenname, dbconn):
		c = dbconn.cursor()
		try:
			c.execute('''CREATE TABLE {} (token TEXT, left TEXT, right TEXT)'''.format(tokenname))
		except sqlite3.OperationalError:
			print(tokenname)
			return
		with open(filename, encoding='utf-8') as data:
			raw_string = data.readline()
			split_string = '{\"rightsize'
			string_list = [split_string + s[:-2] for s in raw_string.split(split_string) if len(s) >= 2]
			for string_dict in string_list:
				try:
					result_dict = ast.literal_eval(string_dict)
				except SyntaxError:
					result_dict = ast.literal_eval(string_dict + '}')
				left_str = [ldict['str'].split(' ') for ldict in result_dict['Left']]
				left_str = [string for sublist in left_str for string in sublist]
				left_str = ' '.join(left_str)
				right_str = [rdict['str'].split(' ') for rdict in result_dict['Right']]
				right_str = [string for sublist in right_str for string in sublist]
				right_str = ' '.join(right_str)
				query = '''INSERT INTO {0} VALUES ('{1}','{2}','{3}')'''.format(tokenname, result_dict['Kwic'][0]['str'], left_str.replace("\'", "\'\'"), right_str.replace("\'", "\'\'"))
				try:
					c.execute(query)
				except sqlite3.OperationalError:
					print(query)
		conn.commit()
		print(tokenname)
	\end{minted}
	
	This function makes up the bulk of the module. It takes a file, a token, and a database connection. It attempts to create a table with the token (e.g. `ar'), and then reads the entire file as a string (because it cannot be loaded as an object due to size). It then splits this raw string at \texttt{'\textbackslash\{\"rightsize'}, which precedes each unique entry in the data. It then evaluates each individual entry as a literal Python object rather than a string before concatenating them (as they are in one or two layers of lists) into a string, which is finally inserted into the database.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	import os
	import json
	import sqlite3
	import ast
	
	datapath = 'data/temp/'
	datadir = os.fsencode(datapath)
	conn = sqlite3.connect('data/data100.db')

	def process_raw_json(filename, tokenname, dbconn):
		...
		
	for file in os.listdir(datadir):
		filename = os.fsdecode(file)
		tokenname = filename[0:-8]
		process_raw_json(datapath+filename, tokenname, conn)
	
	conn.close()
	\end{minted}
	
	The remainder of the file simply iterates through the folder containing all the JSON objects and calls the previously defined function on each of them.
	
	\subsection{\texttt{counter\_task2\_dict.py}}
	
	This module contains task 2, which attempted to extract dative and accusative marked nouns from the sentence.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	import json
	import sqlite3
	
	EXCLUDE_LEFT = ['məli', 'malı', 'mək', 'maq']
	DAT_CASE = ['a', 'ə']
	ACC_CASE = ['i', 'u', 'ü', 'ı']
	EXCLUDE_PREV_WORD = ['in', 'un', 'ün', 'ın']
	END_OF_SENTENCE = ['.', '?', '!', '\n', '<', '>']
	
	conn = sqlite3.connect('data/data100.db')
	c = conn.cursor()
	
	with open('dict_entries_noparens.txt', encoding='utf-8') as f:
		DICT_ENTRIES = set([string[:-1] for string in f.readlines()])
	
	tokens = conn.execute('''SELECT name FROM sqlite_master WHERE type="table"''')
	tokens = [tup[0] for tup in tokens]
	
	pruned_dict_entries = open('pruned_dict_entries.txt', encoding='utf-8', mode='w')
	all_results_pruned = open('all_results_pruned.txt', encoding='utf-8', mode='w')
	no_results_found = open('no_results_found.txt', encoding='utf-8', mode='w')
	\end{minted}
	
	Some definitions, such as case markers, are given. Pruning-related constants, such as the a set of dictionary entries, are created, and a list of all tables to be iterated through are also created. Finally, some log files to track pruning are created.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	def EOS_prune(word_list, kept_words, position):
		if position == len(word_list) - 1:
			return word_list
		elif len(word_list[position]) > 0 and word_list[position][-1] in END_OF_SENTENCE:
			return kept_words
		else:
			kept_words.append(word_list[position])
			return EOS_prune(word_list, kept_words, position+1)
	\end{minted}
	
	This function takes a list of words and returns the same list of words with all words after the sentence boundary removed by recursively iterating until a end of sentence marker, such as a period or an HTML tag (all tags found in a quick sample were paragraph break tags, so we assumed they marked a sentence boundary).
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	def pull_cased_nouns(word_list, case_markers, position, pulled_words):
		if position >= len(word_list) - 1:
			return pulled_words
		curr_word = word_list[position]
		if len(curr_word) >= 3 and curr_word[-1] in case_markers:
			if curr_word.upper() in DICT_ENTRIES:
				pruned_dict_entries.write(word + '\t\t\t\t\t' + curr_word + '\n')
				return pull_cased_nouns(word_list, case_markers, position+1, pulled_words)
			
			if curr_word[-2] == 'n':
				if position >= 1 and word_list[position-1][-2:] in EXCLUDE_PREV_WORD:
				return pull_cased_nouns(word_list, case_markers, position+1, pulled_words)
			elif position == 0:
				return pull_cased_nouns(word_list, case_markers, position+1, pulled_words)
			
			if curr_word[-2:] in ['da', 'də']:
				return pull_cased_nouns(word_list, case_markers, position+1, pulled_words)
			
			pulled_words.append(curr_word)
		return pull_cased_nouns(word_list, case_markers, position+1, pulled_words)
	\end{minted}
	
	This function takes a list of word and extracts nouns marked by the provided case markings according to our pruning rules, and also keeps track of the previous word for this purpose. Pruning rules include being a dictionary entry and the marking of the previous word.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	results_dat = dict()
	results_acc = dict()
	
	for token in tokens:
	token_dat_results = dict()
	token_acc_results = dict()
	
	c.execute('''SELECT token, left, right FROM {0}'''.format(token))
	
	while True:
		r = c.fetchone()
		if r is None:
			break
		word, left, right = r
		
		try:
			if word.endswith(token):
				#exclude word-final appearance
				continue
			
			left = left.split(' ')
			right = right.split(' ')
			
			for left_word in left:
				if any(x in left_word for x in EXCLUDE_LEFT):
					#exclude specific left-occuring words
					raise ValueError
			
			left.reverse()
			#remove words outside of sentence
			left = EOS_prune(left, [], 0)
			left.reverse()
			right = EOS_prune(right, [], 0)
			
		    pulled_dat_list = pull_cased_nouns(left, DAT_CASE, 0, []) + pull_cased_nouns(right, DAT_CASE, 0, [])
			pulled_acc_list = pull_cased_nouns(left, ACC_CASE, 0, []) + pull_cased_nouns(right, ACC_CASE, 0, [])
			
			curr_dat_results = token_dat_results.get(word, {})
			curr_acc_results = token_acc_results.get(word, {})
			
			for pulled_dat in pulled_dat_list:
				curr_dat_results[pulled_dat] = curr_dat_results.get(pulled_dat, 0) + 1
			token_dat_results[word] = curr_dat_results
			
			for pulled_acc in pulled_acc_list:
				curr_acc_results[pulled_acc] = curr_acc_results.get(pulled_acc, 0) + 1
			token_acc_results[word] = curr_acc_results
			
		except ValueError:
			continue
			
	results_dat[token] = token_dat_results
	results_acc[token] = token_acc_results
	
	with open('data/dative_pruned.json', mode='w') as f:
		json.dump(results_dat, f)
	
	with open('data/accusative_pruned.json', mode='w') as f:
		json.dump(results_acc, f)
	
	pruned_dict_entries.close()
	all_results_pruned.close()
	no_results_found.close()
	\end{minted}
	
	This remainder of the module iterates through every table in the database, and for each one, tests one row at a time, extracting appropriately marked nouns. Some basic pruning, such as word-final appearance of the verb marking, is done here. Note that the left-occurring word pruning is incorrect and should be changed if we return to this task in the future. The left and right strings are then passed through the two previously defined functions -- note that the left string is first reversed, as we want to remove the words outside rather than inside of the sentence. The dative and accusative results are separately recorded before being saved in JSON form.
	
	\subsection{\texttt{xlsx\_writer\_task2.py}}
	
	This module takes the JSON files generated by the previous module and writes them to human-readable xlsx format. The reason for this two-step process is that it is much more difficult to work directly with xlsx files than with JSON data, and so it is useful to keep the intermediate JSON representation around.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	import xlsxwriter
	import json
	from operator import itemgetter
	
	workbook_dat = xlsxwriter.Workbook('dict_pruned_dative.xlsx')
	with open('data/dative_pruned.json') as f:
		j_dat = json.load(f)
	
	for token in j_dat:
		worksheet = workbook_dat.add_worksheet(token)
		row = 0
		word_dict = j_dat[token]
		for word in word_dict:
			tup_results = sorted(word_dict[word].items(), key=itemgetter(1), reverse=True)
			for tup in tup_results:
			worksheet.write(row, 0, word)
			worksheet.write(row, 1, tup[0])
			worksheet.write(row, 2, tup[1])
			row += 1
	workbook_dat.close()
	\end{minted}
	
	This module contains the same process for both dative and accusative case (and really should use a function instead of copy-pasting code), so only one of the two is provided due to space. The JSON file is loaded and a sheet is created for each original search query; then, the module iterates through the verbs, sorts their results, then writes them in descending order.
	
	\subsection{\texttt{xlsx\_pruned\_dict\_writer.py}}
	
	This module should really be called `dict\_pruned\_writer', but oh well. This module simply takes the text file of pruned entries, concatenates them by number of occurences, and writes them into an xlsx file for readability.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	import xlsxwriter
	
	with open('pruned_dict_entries.txt', encoding='utf-8') as f:
		lines = f.readlines()
		tuple_list = []
		for line in lines:
		sl = line.split('\t')
		tuple_list.append((sl[0], sl[-1]))
	
	results = {}
	for tup in tuple_list:
		tup_results = results.get(tup[0], {})
		tup_results[tup[1]] = tup_results.get(tup[1], 0) + 1
		results[tup[0]] = tup_results
	
	workbook = xlsxwriter.Workbook('pruned_dict_entries.xlsx')
	worksheet = workbook.add_worksheet('Pruned Entries')
	
	row = 0
	for r in results:
		curr_dict = results[r]
		for key in curr_dict:
			worksheet.write(row, 0, r)
			worksheet.write(row, 1, key)
			worksheet.write(row, 2, curr_dict[key])
			row += 1
	
	workbook.close()
	\end{minted}
	
	The text file of pruned entries is read in. Because of the formatting of that file, we split each line at tabs, resulting in a tuple, from which we take the first and last members. Then, we simply group results and count occurrences before writing the results to an xlsx file.
	
	\subsection{\texttt{task3\_hapax.py}}
	
	This module counts the number of \emph{hapax legomena} occurring within our data set for each causative.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	import sqlite3
	import xlsxwriter
	from operator import itemgetter
	
	with open('task1clean.txt', encoding='utf-8') as f:
		token_list = f.readlines()
	
	conn = sqlite3.connect('data/data100.db')
	
	workbook = xlsxwriter.Workbook('task3_hapax.xlsx')
	\end{minted}
	
	As with previously covered modules, some basic definitions and imports, such as a database connection, are created.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	for token in token_list:
		c = conn.cursor()
		query = '''SELECT token FROM {0}'''.format(token)
		c.execute(query)
		
		total = 0
		hapax = 0
		found_words = dict()
		
		while True:
			row = c.fetchone()
			if row == None:
				break
			found_words[row[0]] = found_words.get(row[0], 0) + 1
		
		for word in found_words:
			num = found_words[word]
			total += num
			if num == 1:
				hapax += 1
		
		results_list = sorted(found_words.items(), key=itemgetter(1), reverse=True)
	\end{minted}
	
	The module then iterates through all tables in the database, and fetches the verb (`\texttt{word}') from each row, keeping count of the number of times each verb occurs. Afterward, it iterates through these results and calculates both the total number of verbs as well as the number of hapaxes.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	worksheet = workbook.add_worksheet(token)
	
	row = 0
	worksheet.write(row, 0, 'Total hapax')
	worksheet.write(row, 1, 'Total tokens')
	
	row += 1
	worksheet.write(row, 0, hapax)
	worksheet.write(row, 1, total)
	worksheet.write(row, 2, str(hapax/total))
	
	row += 2
	for result in results_list:
		worksheet.write(row, 0, result[0])
		worksheet.write(row, 1, result[1])
		row += 1
	
	\end{minted}
	
	It then writes this into an xlsx file; no intermediate representation is used as currently the intermediate data is not expected to be needed in the immediate future.
	
	\subsection{\texttt{syntactic\_category\_fetch.py}}
	
	This module goes back into the dictionary to try to extract the syntactic category of each entry.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	import string
	import re
	import xlsxwriter
	import json
	
	INVALID_EXCEL_CHAR = r'[\[\]\:\*\?\/\\]'
	
	with open('dict.txt', encoding='utf-8') as f:
	
		res = dict()
		curr = ''
		
		for l in f:
			line = l.split(' ')
			
			try:
				if line[0].isupper():
					curr = re.sub(r'\([^()]*\)', '', line[0])
					line = line[1:]
				
				for word in line:
					if word.islower() and '.' in word and not any([digit in word for digit in string.digits]):
						word = re.sub(INVALID_EXCEL_CHAR, '', word)
						rlist = res.get(word, [])
						rlist.append(curr)
						res[word] = rlist
				else:
					raise IndexError
		
		except IndexError:
			continue
	\end{minted}
	
	This module takes much the same approach as \texttt{simple\_get\_dict\_entry.py}. It looks for strings resembling abbreviations for parts of speech, which in the dictionary seemed to take the form of lowercase letters with at least one period. For each of these potential syntactic category markers, all entries found with it are kept track of. This method produces a lot of chaff, but has the advantage of being simple and fairly likely to be complete.
	
	\begin{minted}[bgcolor=bg, breaklines=true, obeytabs=true, tabsize=4]{python}
	with open('../data/syncat.json', encoding='utf-8', mode='w') as f:
		json.dump(res, f)
	
	workbook = xlsxwriter.Workbook('syntactic_category.xlsx')
	for key in res:
		row = 0
		if len(res[key]) < 20:
			continue
		worksheet = workbook.add_worksheet(key)
		for entry in res[key]:
			worksheet.write(row, 0, entry)
			row += 1
	workbook.close()
	\end{minted}
	
	The data is then stored in a JSON file as well as written to an xlsx file.
	
\end{document}               % End of document.